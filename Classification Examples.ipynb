{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad1572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938708c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df4018",
   "metadata": {},
   "source": [
    "## Load Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f650d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('data/tweets-examples.tsv', sep='\\t', header=None, names=['id', 'message'])\n",
    "tweets['id'] = tweets['id'].apply(lambda url: url.split('/')[-1])\n",
    "tweets.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b7598",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7955ba",
   "metadata": {},
   "source": [
    "## `vaderSentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c5a6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def find_sentiment(tweet):\n",
    "    sentiment = analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    if abs(sentiment['compound']) <= 0.05:\n",
    "        return 'neutral'\n",
    "    \n",
    "    if sentiment['compound'] > 0:\n",
    "        return 'positive'\n",
    "    \n",
    "    return 'negative'\n",
    "\n",
    "tweets['vader'] = tweets['message'].apply(find_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c7691",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59836f0",
   "metadata": {},
   "source": [
    "# Textblob - `PatternAnalyzer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64256fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Blobber\n",
    "from textblob.sentiments import PatternAnalyzer\n",
    "\n",
    "blobber = Blobber(analyzer=PatternAnalyzer())\n",
    "\n",
    "def find_sentiment(tweet):\n",
    "    sentiment = blobber(tweet).sentiment\n",
    "    if abs(sentiment.polarity) <= 0.05:\n",
    "        return 'neutral'\n",
    "    \n",
    "    if sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    \n",
    "    return 'negative'\n",
    "\n",
    "tweets['textblob'] = tweets['message'].apply(find_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a376d43",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60c917",
   "metadata": {},
   "source": [
    "## BERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bd4904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)\n",
    "bertweet = AutoModelForSequenceClassification.from_pretrained(\"checkpoints/checkpoint-2500/\", num_labels=3)\n",
    "\n",
    "input_ids = torch.LongTensor(tokenizer(list(tweets['message']), padding=True)['input_ids'])\n",
    "outputs = bertweet(input_ids)\n",
    "predictions = torch.argmax(torch.nn.functional.softmax(torch.Tensor(outputs[0])), dim=1).tolist()\n",
    "\n",
    "idx2label = {0: 'neutral', 1: 'positive', 2: 'negative'}\n",
    "tweets['bertweet'] = [idx2label[pred] for pred in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10064e45",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ee386",
   "metadata": {},
   "source": [
    "## $n$-gram Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0a438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Clean and prepare messages:\n",
    "eyes_regex = r'[8:=;]'\n",
    "nose_regex = r\"['`\\-]?\"\n",
    "\n",
    "def replace_hash_text(match):\n",
    "    hash_text = match.group(1)\n",
    "    if hash_text.isupper():\n",
    "        return '<HASHTAG> ' + hash_text\n",
    "    else:\n",
    "        return '<HASHTAG> ' + ' '.join(re.findall(r'([a-zA-Z0-9]+?)(?=\\b|[A-Z0-9_])', hash_text))\n",
    "\n",
    "tweets['processed'] = tweets['message']\n",
    "tweets['processed'] = tweets['processed'].str.decode('unicode_escape', errors='ignore')\n",
    "tweets['processed'] = tweets['processed'].str.strip('\"')  # remove left-most and right-most \"\n",
    "tweets['processed'] = tweets['processed'].str.replace('\"\"', '\"', regex=False)\n",
    "tweets['processed'] = tweets['processed'].str.replace(r'https?://\\S+\\b|www\\.(\\w+\\.)+\\S*', '<URL>') # replace URLs\n",
    "tweets['processed'] = tweets['processed'].str.replace(r'([/()\\[\\]])',r' \\1 ') # Force splitting words appended with slashes/parenthesis/brackets\n",
    "tweets['processed'] = tweets['processed'].str.replace(r'@\\w+', '<USER>') # Replace usernames\n",
    "tweets['processed'] = tweets['processed'].str.replace(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*', ' <NUMBER> ') # Replace numbers\n",
    "tweets['processed'] = tweets['processed'].str.replace(r'#(\\S+)', replace_hash_text)\n",
    "tweets['processed'] = tweets['processed'].str.replace(eyes_regex + nose_regex + r'[)d]+|[)d]+' + nose_regex + eyes_regex, '<SMILE>', flags=re.IGNORECASE)\n",
    "tweets['processed'] = tweets['processed'].str.replace(eyes_regex + nose_regex + r'p+', '<LOLFACE>', flags=re.IGNORECASE)\n",
    "tweets['processed'] = tweets['processed'].str.replace(eyes_regex + nose_regex + r'\\(+|\\)+' + nose_regex + eyes_regex, '<SADFACE>')\n",
    "tweets['processed'] = tweets['processed'].str.replace(eyes_regex + nose_regex + r'[/|l*]', '<NEUTRALFACE>')\n",
    "tweets['processed'] = tweets['processed'].str.replace(r'<3', '<HEART>')\n",
    "tweets['processed'] = tweets['processed'].str.replace(r'([!?.]){2,}', r'\\1 <REPEAT>') # Mark punctuation repetitions (eg. \"!!!\" => \"! <REPEAT>\")\n",
    "tweets['processed'] = tweets['processed'].str.replace(r'\\b(\\S*?)(.)\\2{2,}\\b', r'\\1\\2 <ELONG>') # Mark elongated words (eg. \"wayyyy\" => \"way <ELONG>\")\n",
    "tweets['processed'] = tweets['processed'].str.replace(r'\\s+', ' ') # Replace all whitespace characters by only one space\n",
    "tweets['processed'] = tweets['processed'].str.strip()\n",
    "tweets['processed'] = tweets['processed'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a83121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "def features(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    unigram = dict([(token, True) for token in tokens])\n",
    "    bigram = dict([(' '.join(tokens), True) for tokens in nltk.bigrams(tokens)])\n",
    "    trigram = dict([(' '.join(tokens), True) for tokens in nltk.trigrams(tokens)])\n",
    "    \n",
    "    return {**unigram, **bigram, **trigram}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5308bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('nb-classifier.pickle', 'rb') as handle:\n",
    "    classifier = pickle.load(handle)\n",
    "\n",
    "tweets['nb'] = tweets['processed'].apply(features).apply(classifier.classify)\n",
    "tweets.drop(columns=['processed'], inplace=True)\n",
    "\n",
    "# classifier.show_most_informative_features(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006a395",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f6326f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>vader</th>\n",
       "      <th>textblob</th>\n",
       "      <th>bertweet</th>\n",
       "      <th>nb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1373967439944425475</th>\n",
       "      <td>Why the EU Should Not Create a Separate AI Pro...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383337183013072904</th>\n",
       "      <td>Delighted to see this finally published: Trust...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395764999029657607</th>\n",
       "      <td>Great congratulations to Bashar Nuseibeh @BNus...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397891830306512899</th>\n",
       "      <td>I’m not enjoying this spider! #spider</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396373953208332288</th>\n",
       "      <td>Well done UK, we did ourselves proud once agai...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395686278813859845</th>\n",
       "      <td>So excited to have @TLevingstone at our #CRTAI...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387447572256657414</th>\n",
       "      <td>We mourn the passing of Apollo 11 astronaut Mi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397881733937238016</th>\n",
       "      <td>I really enjoyed talking to the amazing &amp; bril...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395788559953670154</th>\n",
       "      <td>Thank you for the opportunity to speak to the ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               message  \\\n",
       "id                                                                       \n",
       "1373967439944425475  Why the EU Should Not Create a Separate AI Pro...   \n",
       "1383337183013072904  Delighted to see this finally published: Trust...   \n",
       "1395764999029657607  Great congratulations to Bashar Nuseibeh @BNus...   \n",
       "1397891830306512899              I’m not enjoying this spider! #spider   \n",
       "1396373953208332288  Well done UK, we did ourselves proud once agai...   \n",
       "1395686278813859845  So excited to have @TLevingstone at our #CRTAI...   \n",
       "1387447572256657414  We mourn the passing of Apollo 11 astronaut Mi...   \n",
       "1397881733937238016  I really enjoyed talking to the amazing & bril...   \n",
       "1395788559953670154  Thank you for the opportunity to speak to the ...   \n",
       "\n",
       "                        vader  textblob  bertweet        nb  \n",
       "id                                                           \n",
       "1373967439944425475  negative   neutral   neutral  negative  \n",
       "1383337183013072904  positive  positive  positive  positive  \n",
       "1395764999029657607  positive  positive  positive  positive  \n",
       "1397891830306512899  negative  negative  negative  positive  \n",
       "1396373953208332288  positive  positive  positive  negative  \n",
       "1395686278813859845  negative  positive  positive  positive  \n",
       "1387447572256657414  positive  positive  positive  negative  \n",
       "1397881733937238016  positive  positive  positive  positive  \n",
       "1395788559953670154  positive  positive  positive  positive  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b93f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.set_index(map(lambda s_id: 'https://twitter.com/profile/status/'+str(s_id), \n",
    "                     list(tweets.index)), \n",
    "                 inplace=True)\n",
    "\n",
    "tweets.to_csv('outputs/example-predictions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d9853c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
